# clients/ggbases_client.py
# è¯¥æ¨¡å—ç”¨äºä¸ GGBases ç½‘ç«™äº¤äº’ï¼Œè·å–æ¸¸æˆä¿¡æ¯å’Œæ ‡ç­¾
import os
import urllib.parse

import requests
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from utils.tag_logger import append_new_tags

TAG_GGBASE_PATH = os.path.join(os.path.dirname(__file__), "..", "mapping", "tag_ggbase.json")


class GGBasesClient:
    BASE_URL = "https://ggbases.dlgal.com"

    def __init__(self):
        self.driver = None
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
            }
        )
        self._cache = {}

    def set_driver(self, driver):
        self.driver = driver

    def choose_or_parse_popular_url_with_requests(self, keyword):
        print(f"ğŸ” [GGBases] æ­£åœ¨é€šè¿‡ requests æœç´¢: {keyword}")
        try:
            encoded = urllib.parse.quote(keyword)
            search_url = f"{self.BASE_URL}/search.so?p=0&title={encoded}&advanced="
            resp = self.session.get(search_url, timeout=15)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "lxml")
            rows = soup.find_all("tr", class_="dtr")
            candidates = []

            for i, row in enumerate(rows[:10]):
                detail_link = row.find("a", href=lambda x: x and "/view.so?id=" in x)
                if not detail_link:
                    continue
                url = urllib.parse.urljoin(self.BASE_URL, detail_link["href"])

                title = (
                    row.find_all("td")[1].get_text(separator=" ", strip=True)
                    if len(row.find_all("td")) > 1
                    else "æ— æ ‡é¢˜"
                )

                popularity = 0
                pop_a = row.select_one("a.l-a span")
                if pop_a and pop_a.get_text(strip=True).isdigit():
                    popularity = int(pop_a.get_text(strip=True))

                candidates.append({"title": title, "url": url, "popularity": popularity})

            if not candidates:
                print("âš ï¸ [GGBases] (requests) æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆç»“æœ")
                return None

            best = max(candidates, key=lambda x: x["popularity"])
            print(f"ğŸ”¥ [GGBases] (requests) è‡ªåŠ¨é€‰æ‹©çƒ­åº¦æœ€é«˜ç»“æœ: {best['title']} ({best['popularity']})")
            return best["url"]

        except requests.RequestException as e:
            print(f"âŒ [GGBases] (requests) æœç´¢è¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ [GGBases] (requests) è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
            return None

    def get_info_by_url_with_selenium(self, detail_url):
        if not self.driver:
            raise RuntimeError("GGBasesClientçš„Selenium driveræœªè®¾ç½®ï¼Œæ— æ³•æ‰§è¡ŒJSæ¸²æŸ“æŠ“å–ã€‚")
        if not detail_url:
            return {}

        print(f"ğŸ”© [GGBases] æ­£åœ¨å¯åŠ¨SeleniumæŠ“å–è¯¦æƒ…é¡µ: {detail_url}")
        try:
            self.driver.get(detail_url)
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*="tags.so?target=female"]'))
            )
            soup = BeautifulSoup(self.driver.page_source, "lxml")
            info = {
                "å®¹é‡": self._extract_game_size(soup),
                "å°é¢å›¾é“¾æ¥": self._extract_cover_url(soup),
                "æ ‡ç­¾": self._extract_tags(soup),
            }
            return info
        except Exception as e:
            print(f"âš ï¸ [GGBases] (Selenium) æŠ“å–è¯¦æƒ…é¡µå¤±è´¥: {e}")
            return {}

    def _normalize_url(self, src):
        if not src or src.startswith("data:"):
            return None
        if src.startswith("//"):
            return "https:" + src
        elif src.startswith("/"):
            return urllib.parse.urljoin(self.BASE_URL, src)
        return src

    def _extract_game_size(self, soup):
        size_td = soup.select_one('td:-soup-contains("å¤§å°"), td:-soup-contains("å®¹é‡")')
        if size_td and (span := size_td.find("span", class_="label")):
            return span.get_text(strip=True)
        return None

    def _extract_cover_url(self, soup):
        img = soup.select_one("div[markdown-text] img, #img00")
        if img and img.get("src"):
            return self._normalize_url(img["src"])
        a_tag = soup.select_one("div[markdown-text] a[href]")
        if a_tag:
            return self._normalize_url(a_tag["href"])
        return None

    def _extract_tags(self, soup):
        female_tags = [
            span.get_text(strip=True)
            for tr in soup.find_all("tr")
            if tr.find("a", href=lambda x: x and "tags.so?target=female" in x)
            for span in tr.find_all("span", class_="female_span")
        ]
        all_tags = female_tags
        if all_tags:
            append_new_tags(TAG_GGBASE_PATH, all_tags)
        return all_tags
