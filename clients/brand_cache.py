# clients/brand_cache.py
# è¯¥æ¨¡å—ç”¨äºå¤„ç†å“ç‰Œä¿¡æ¯çš„ç¼“å­˜
import json
import os
import shutil
from datetime import datetime
import hashlib

CACHE_DIR = "cache"
CACHE_FILE_NAME = "brand_extra_info_cache.json"
CACHE_FILE = os.path.join(CACHE_DIR, CACHE_FILE_NAME)


class BrandCache:
    def __init__(self, cache_file=CACHE_FILE):
        self.cache_file = cache_file
        os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)

    def load_cache(self):
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.last_cache_hash = self._hash_content(data)
                    print(f"â™»ï¸ å·²åŠ è½½å“ç‰Œç¼“å­˜ {len(data)} æ¡")
                    return data
            except Exception as e:
                print(f"âš ï¸ è¯»å–å“ç‰Œç¼“å­˜å¤±è´¥: {e}")
        self.last_cache_hash = None
        return {}

    def save_cache(self, cache: dict):
        try:
            if not cache:
                print("âš ï¸ æ£€æµ‹åˆ°ç¼“å­˜æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜ï¼Œé¿å…è¦†ç›–åŸæœ‰æ•°æ®")
                return

            new_hash = self._hash_content(cache)
            if new_hash == getattr(self, "last_cache_hash", None):
                print("â„¹ï¸ ç¼“å­˜å†…å®¹æ— å˜åŒ–ï¼Œè·³è¿‡ä¿å­˜")
                return

            # 1. å¤‡ä»½æ—§ç¼“å­˜ï¼ˆæ¯å¤©æœ€å¤šä¸€ä¸ªï¼‰
            if os.path.exists(self.cache_file):
                date_str = datetime.now().strftime("%Y%m%d")
                backup_file = os.path.join(
                    os.path.dirname(self.cache_file),
                    f"{CACHE_FILE_NAME}.bak_{date_str}",
                )
                if not os.path.exists(backup_file):
                    shutil.copy2(self.cache_file, backup_file)
                    print(f"ğŸ“¦ å·²å¤‡ä»½æ—§ç¼“å­˜ä¸º: {backup_file}")
                else:
                    print(f"ğŸ“¦ ä»Šæ—¥å¤‡ä»½å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤å¤‡ä»½")

            # 2. å†™å…¥ä¸´æ—¶æ–‡ä»¶
            tmp_file = self.cache_file + ".tmp"
            with open(tmp_file, "w", encoding="utf-8") as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)

            # 3. åŸå­æ›¿æ¢
            os.replace(tmp_file, self.cache_file)

            # 4. ä¿å­˜å½“å‰ hashï¼Œé¿å…é‡å¤ä¿å­˜
            self.last_cache_hash = new_hash

            print(f"ğŸ’¾ å·²ä¿å­˜å“ç‰Œç¼“å­˜ {len(cache)} æ¡")

            # 5. æ¸…ç†æ—§å¤‡ä»½ï¼ˆä»…ä¿ç•™æœ€è¿‘5å¤©ï¼‰
            self._cleanup_old_backups()

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å“ç‰Œç¼“å­˜å¤±è´¥: {e}")

    def _cleanup_old_backups(self, keep_last_n=5):
        """ä»…ä¿ç•™æœ€è¿‘ N å¤©çš„å¤‡ä»½ï¼ŒæŒ‰æ—¥æœŸæ’åºæ¸…ç†"""
        dir_path = os.path.dirname(self.cache_file)
        prefix = f"{CACHE_FILE_NAME}.bak_"
        backup_files = [f for f in os.listdir(dir_path) if f.startswith(prefix)]

        # æå–æ—¥æœŸå¹¶æ’åº
        dated_backups = []
        for fname in backup_files:
            try:
                date_str = fname.replace(prefix, "")
                date_obj = datetime.strptime(date_str, "%Y%m%d")
                dated_backups.append((date_obj, fname))
            except ValueError:
                continue

        # æŒ‰æ—¥æœŸé™åºä¿ç•™æœ€æ–° N ä¸ª
        dated_backups.sort(reverse=True)
        for _, old_file in dated_backups[keep_last_n:]:
            try:
                os.remove(os.path.join(dir_path, old_file))
                print(f"ğŸ§¹ å·²æ¸…ç†æ—§å¤‡ä»½: {old_file}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åˆ é™¤æ—§å¤‡ä»½ {old_file}: {e}")

    def _hash_content(self, data: dict) -> str:
        """å¯¹å­—å…¸å†…å®¹è¿›è¡Œå“ˆå¸Œï¼Œä»¥åˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–"""
        try:
            serialized = json.dumps(data, sort_keys=True)
            return hashlib.md5(serialized.encode("utf-8")).hexdigest()
        except Exception:
            return ""
