# clients/dlsite_client.py
import os
import sys
import contextlib
import re
import requests
import urllib.parse
from bs4 import BeautifulSoup
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from utils.tag_logger import append_new_tags
import undetected_chromedriver as uc

TAG_JP_PATH = os.path.join(os.path.dirname(__file__), "..", "mapping", "tag_jp_to_cn.json")

@contextlib.contextmanager
def suppress_stdout_stderr():
    """
    é‡å®šå‘stdoutå’Œstderråˆ°nullï¼Œå±è”½æµè§ˆå™¨å¯åŠ¨æ—¶æ—¥å¿—ã€‚
    """
    with open(os.devnull, 'w') as devnull:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        sys.stdout = devnull
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

def create_silent_uc_driver():
    # é™ä½tensorflowã€CUDAç­‰åº“çš„æ—¥å¿—
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
    os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--blink-settings=imagesEnabled=false")
    options.add_argument("--window-size=1280,1024")

    # Chromeæ—¥å¿—çº§åˆ«å‚æ•°
    options.add_argument("--log-level=3")
    options.add_argument("--silent")
    options.add_experimental_option("excludeSwitches", ["enable-logging", "enable-automation"])

    with suppress_stdout_stderr():
        driver = uc.Chrome(options=options)
    return driver


class DlsiteClient:
    BASE_URL = "https://www.dlsite.com"

    def __init__(self, headers=None, driver=None):
        self.headers = headers or {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.dlsite.com/maniax/"
        }
        self.driver = driver
        self.external_driver = driver is not None
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search(self, keyword, limit=30):
        print(f"ğŸ” [Dlsite] æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
        query = urllib.parse.quote_plus(keyword)
        url = (
            f"{self.BASE_URL}/maniax/fsr/=/language/jp/"
            f"sex_category%5B0%5D/male/"
            f"keyword/{query}/"
            f"work_category%5B0%5D/doujin/"
            f"work_category%5B1%5D/books/"
            f"work_category%5B2%5D/pc/"
            f"work_category%5B3%5D/app/"
            f"order%5B0%5D/trend/"
            f"options_and_or/and/"
            f"per_page/30/"
            f"page/1/"
            f"from/fs.header"
        )
        r = self.session.get(url, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        results = []
        seen = set()

        for a in soup.select("a[href*='/work/=/product_id/']"):
            container = a.find_parent()
            for _ in range(3):  # å‘ä¸Šæ‰¾ price å®¹å™¨
                if container is None:
                    break
                price_tag = container.select_one(".price, .work_price, .price_display")
                if price_tag:
                    break
                container = container.parent
            price = price_tag.get_text(strip=True) if price_tag else "æ— "

            title = a.get("title", "").strip()
            href = a["href"]
            full_url = href if href.startswith("http") else self.BASE_URL + href

            # æŸ¥æ‰¾ä½œå“ç±»å‹ï¼ˆå¦‚ï¼šå‹•ç”»ã€CGãƒ»ã‚¤ãƒ©ã‚¹ãƒˆç­‰ï¼‰
            li_tag = a.find_parent("li", class_="search_result_img_box_inner")
            work_type_tag = li_tag.select_one(".work_category a") if li_tag else None
            work_type = work_type_tag.get_text(strip=True) if work_type_tag else None

            if title and full_url and full_url not in seen:
                results.append({
                    "title": title,
                    "url": full_url,
                    "price": price,
                    "ç±»å‹": work_type  # âœ… æ–°å¢å­—æ®µ
                })
                seen.add(full_url)

            if len(results) >= limit:
                break
        
        # === åªæ’é™¤éæ¸¸æˆç±»åˆ«ï¼Œä¿ç•™å…¶ä½™ ===
        exclude_keywords = ["å˜è¡Œæœ¬", "ãƒãƒ³ã‚¬", "å°èª¬", "æ›¸ç±", "é›‘èªŒ/ã‚¢ãƒ³ã‚½ãƒ­", "ãƒœã‚¤ã‚¹ãƒ»ASMR", "éŸ³æ¥½"]

        filtered_results = []
        for item in results:
            item_type = item.get("ç±»å‹", "")
            if not any(ex_kw in item_type for ex_kw in exclude_keywords):
                filtered_results.append(item)

        print(f"âœ… [Dlsite] ç­›é€‰åæ‰¾åˆ° {len(filtered_results)} æ¡æ¸¸æˆç›¸å…³ç»“æœ")
        return filtered_results

    def get_game_detail(self, url):
        r = self.session.get(url, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        brand_tag = soup.select_one("#work_maker .maker_name a")
        brand = brand_tag.get_text(strip=True) if brand_tag else None
        brand_page_url = brand_tag["href"] if brand_tag and brand_tag.has_attr("href") else None
        if brand_page_url and not brand_page_url.startswith("http"):
            brand_page_url = self.BASE_URL + brand_page_url

        sale_date, scenario, illustrator, voice_actor, music, genres, work_type = None, [], [], [], [], [], []
        table = soup.find("table", id="work_outline")
        if table:
            for tr in table.find_all("tr"):
                th, td = tr.find("th"), tr.find("td")
                if not th or not td:
                    continue
                key = th.get_text(strip=True)
                val = [a.get_text(strip=True) for a in td.find_all("a")]
                if key == "è²©å£²æ—¥":
                    sale_date = td.get_text(strip=True)
                elif key == "ã‚·ãƒŠãƒªã‚ª":
                    scenario = val
                elif key == "ã‚¤ãƒ©ã‚¹ãƒˆ":
                    illustrator = val
                elif key == "å£°å„ª":
                    voice_actor = val
                elif key == "éŸ³æ¥½":
                    music = val
                elif key == "ã‚¸ãƒ£ãƒ³ãƒ«":
                    genres = val
                elif key == "ä½œå“å½¢å¼":
                    mapping = {
                        "ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ³ã‚°": "RPG",
                        "ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼": "ADV",
                        "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "æ¨¡æ‹Ÿ",
                        "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": "ACT",
                        "éŸ³å£°ã‚ã‚Š": "æœ‰å£°éŸ³",
                        "éŸ³æ¥½ã‚ã‚Š": "æœ‰éŸ³ä¹",
                        "å‹•ç”»ã‚ã‚Š": "æœ‰åŠ¨ç”»",
                    }
                    work_type = [
                        mapping.get(s.get("title", "").strip(), s.get("title", "").strip())
                        for s in td.find_all("span") if s.has_attr("title")
                    ]
        cover = soup.find("meta", property="og:image")
        cover = cover["content"] if cover and cover.has_attr("content") else None

        # âœ… æ–°å¢ï¼šæå–æ¸¸æˆå®¹é‡ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡ï¼‰
        capacity = None
        capacity_th = soup.find("th", string=lambda s: s and "ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡" in s)
        if capacity_th:
            td = capacity_th.find_next_sibling("td")
            if td:
                text = td.get_text(strip=True)
                # æå–åƒ "3.46GB" æˆ– "356.59MB" çš„å®¹é‡æ•°å€¼
                match = re.search(r'([\d.]+(?:MB|GB))', text)
                if match:
                    capacity = match.group(1)

        # âœ… å°†æŠ“å–åˆ°çš„æ ‡ç­¾å†™å…¥ tag_jp_to_cn æ˜ å°„æ¨¡å—
        if genres:
            append_new_tags(TAG_JP_PATH, genres)

        return {
            "å“ç‰Œ": brand,
            "å‘å”®æ—¥": sale_date,
            "å‰§æœ¬": scenario,
            "åŸç”»": illustrator,
            "å£°ä¼˜": voice_actor,
            "éŸ³ä¹": music,
            "æ ‡ç­¾": genres,
            "ä½œå“å½¢å¼": work_type,
            "å°é¢å›¾é“¾æ¥": cover,
            "å“ç‰Œé¡µé“¾æ¥": brand_page_url,
            "å®¹é‡": capacity,
        }

    def batch_get_brand_extra_info_from_dlsite(self, brand_page_urls):
        print(f"â³ [Dlsite] æ‰¹é‡è·å–å“ç‰Œé¢å¤–ä¿¡æ¯ï¼Œæ•°é‡: {len(brand_page_urls)}")

        driver = self.driver or create_silent_uc_driver()
        wait = WebDriverWait(driver, 2)

        results = {}

        try:
            for url in brand_page_urls:
                try:
                    driver.get(url)
                    link_cien_present = False
                    try:
                        link_cien_present = wait.until(
                            lambda d: len(d.find_elements(By.CSS_SELECTOR, "div.link_cien")) > 0
                        )
                    except:
                        pass

                    if not link_cien_present:
                        print(f"âš ï¸ [Dlsite] æœªæ‰¾åˆ° link_cien åŒºå—ï¼ˆè·³è¿‡ï¼‰: {url}")
                        results[url] = {"å®˜ç½‘": None, "å›¾æ ‡": None}
                        continue

                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    link_block = soup.select_one("div.link_cien")
                    cien_link = link_block.select_one("a[href]")
                    icon_img = link_block.select_one("img[src]")

                    official_url = cien_link["href"].strip() if cien_link else None
                    icon_url = icon_img["src"].strip() if icon_img else None

                    results[url] = {"å®˜ç½‘": official_url, "å›¾æ ‡": icon_url}
                    print(f"âœ… [Dlsite] è·å–æˆåŠŸ: å®˜ç½‘={official_url}, å›¾æ ‡={icon_url}")
                except Exception as e:
                    print(f"âŒ [Dlsite] æŠ“å–å¤±è´¥ {url}: {e}")
                    results[url] = {"å®˜ç½‘": None, "å›¾æ ‡": None}
        finally:
            if not self.external_driver and driver:
                driver.quit()
                print(f"ğŸ§¹ [Dlsite] å…³é—­å†…éƒ¨æµè§ˆå™¨é©±åŠ¨")
        return results

    def get_brand_extra_info_from_dlsite(self, brand_page_url):
        print(f"ğŸŒ [Dlsite] è·å–å“ç‰Œå®˜ç½‘ä¸å›¾æ ‡: {brand_page_url}")
        if not brand_page_url:
            return {"å®˜ç½‘": None, "å›¾æ ‡": None}

        driver_created = False

        try:
            driver = self.driver
            if not driver:
                driver = create_silent_uc_driver()
                driver_created = True

            driver.get(brand_page_url)

            page_source = driver.page_source
            if "link_cien" not in page_source:
                print(f"âš ï¸ [Dlsite] é¡µé¢ä¸­æœªå‘ç° link_cien")
                return {"å®˜ç½‘": None, "å›¾æ ‡": None}

            soup = BeautifulSoup(page_source, "html.parser")
            link_block = soup.select_one("div.link_cien")
            if not link_block:
                print(f"âš ï¸ [Dlsite] æ—  link_cien å—")
                return {"å®˜ç½‘": None, "å›¾æ ‡": None}

            cien_link = link_block.select_one("a[href]")
            icon_img = link_block.select_one("img[src]")

            official_url = cien_link["href"].strip() if cien_link else None
            icon_url = icon_img["src"].strip() if icon_img else None

            print(f"âœ… [Dlsite] è·å–æˆåŠŸ: å®˜ç½‘={official_url}, å›¾æ ‡={icon_url}")
            return {"å®˜ç½‘": official_url, "å›¾æ ‡": icon_url}

        except Exception as e:
            print(f"âŒ [Dlsite] è·å–å“ç‰Œé¢å¤–ä¿¡æ¯å¤±è´¥: {e}")
            return {"å®˜ç½‘": None, "å›¾æ ‡": None}
        finally:
            if driver_created:
                driver.quit()
                print(f"ğŸ§¹ [Dlsite] å…³é—­å†…éƒ¨æµè§ˆå™¨é©±åŠ¨ï¼ˆå•æ¬¡ï¼‰")
