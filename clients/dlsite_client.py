# clients/dlsite_client.py
import asyncio
import logging
import os
import traceback
import urllib.parse

from bs4 import BeautifulSoup
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium_stealth import stealth

from utils.tag_logger import append_new_tags

from .base_client import BaseClient

TAG_JP_PATH = os.path.join(os.path.dirname(__file__), "..", "mapping", "tag_jp_to_cn.json")


class DlsiteClient(BaseClient):
    def __init__(self, client):
        super().__init__(client, base_url="https://www.dlsite.com")
        self.headers.update({
            "Referer": "https://www.dlsite.com/maniax/",
        })
        self.driver = None
        self.selenium_timeout = 5

    def set_driver(self, driver):
        self.driver = driver
        stealth(
            self.driver,
            languages=["ja-JP", "ja"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
        )

    def has_driver(self):
        """æ£€æŸ¥æ˜¯å¦å·²è®¾ç½®é©±åŠ¨ç¨‹åºã€‚"""
        return self.driver is not None

    async def search(self, keyword, limit=30):
        logging.info(f"ğŸ” [Dlsite] æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
        query = urllib.parse.quote_plus(keyword)
        url = f"/maniax/fsr/=/language/jp/sex_category%5B0%5D/male/keyword/{query}/work_category%5B0%5D/doujin/work_category%5B1%5D/books/work_category%5B2%5D/pc/work_category%5B3%5D/app/order%5B0%5D/trend/options_and_or/and/per_page/30/page/1/from/fs.header"

        resp = await self.get(url, timeout=15)
        if not resp:
            return []

        soup = BeautifulSoup(resp.text, "html.parser")
        results = []
        seen = set()

        for li in soup.select("li.search_result_img_box_inner"):
            if len(results) >= limit:
                break

            title_a = li.select_one(".work_name a")
            if not title_a:
                continue

            href = title_a.get("href")
            if not href:
                continue
            
            full_url = href if href.startswith("http") else self.base_url + href
            if full_url in seen:
                continue
            
            title = title_a.get("title", "").strip()
            if not title:
                continue

            price_tag = li.select_one(".work_price, .price_display")
            price = price_tag.get_text(strip=True) if price_tag else "æ— "

            work_type_tag = li.select_one(".work_category a")
            work_type = work_type_tag.get_text(strip=True) if work_type_tag else None

            thumbnail_url = None
            img_tag = li.select_one("img.lazy")
            if img_tag:
                thumbnail_url = img_tag.get('data-src') or img_tag.get('src')

            results.append({
                "title": title,
                "url": full_url,
                "price": price,
                "ç±»å‹": work_type,
                "thumbnail_url": thumbnail_url
            })
            seen.add(full_url)

        exclude_keywords = [
            "å˜è¡Œæœ¬", "ãƒãƒ³ã‚¬", "å°èª¬", "æ›¸ç±", "é›‘èªŒ/ã‚¢ãƒ³ã‚½ãƒ­",
            "ãƒœã‚¤ã‚¹ãƒ»ASMR", "éŸ³æ¥½", "å‹•ç”»", "CGãƒ»ã‚¤ãƒ©ã‚¹ãƒˆ", "å˜è©±",
        ]
        filtered_results = [
            item
            for item in results
            if not any(ex_kw in (item.get("ç±»å‹") or "") for ex_kw in exclude_keywords)
        ]
        logging.info(f"âœ… [Dlsite] ç­›é€‰åæ‰¾åˆ° {len(filtered_results)} æ¡æ¸¸æˆç›¸å…³ç»“æœ")
        return filtered_results

    async def get_game_detail(self, url):
        resp = await self.get(url, timeout=15, headers={"Cookie": "adultchecked=1;"})
        if not resp:
            return {}
        try:
            soup = BeautifulSoup(resp.text, "lxml")
            brand_tag = soup.select_one("#work_maker .maker_name a")
            brand = brand_tag.get_text(strip=True) if brand_tag else None
            brand_page_url = brand_tag["href"] if brand_tag and brand_tag.has_attr("href") else None
            if brand_page_url and not brand_page_url.startswith("http"):
                brand_page_url = self.base_url + brand_page_url

            details = {}

            table = soup.find("table", id="work_outline")
            if table:
                for tr in table.find_all("tr"):
                    th, td = tr.find("th"), tr.find("td")
                    if not th or not td:
                        continue
                    key = th.get_text(strip=True)

                    def extract_list_from_td(table_cell):
                        # Replace <br> tags with a common separator
                        for br in table_cell.find_all("br"):
                            br.replace_with(",")

                        # Get all text, using the common separator
                        all_text = table_cell.get_text(separator=",")

                        # Standardize all separators to the common one and then split
                        processed_text = all_text.replace('ã€', ',').replace('/', ',').replace('ï¼Œ', ',')
                        return [name.strip() for name in processed_text.split(',') if name.strip()]

                    if key in self.STAFF_MAPPING:
                        details[self.STAFF_MAPPING[key]] = extract_list_from_td(td)
                    elif key == "è²©å£²æ—¥":
                        details["å‘å”®æ—¥"] = td.get_text(strip=True)
                    elif key == "ã‚¸ãƒ£ãƒ³ãƒ«":
                        details["æ ‡ç­¾"] = [a.get_text(strip=True) for a in td.find_all("a")]
                    elif key == "ä½œå“å½¢å¼":
                        spans = td.find_all("span", title=True)
                        details["ä½œå“å½¢å¼"] = [
                            self._genre_reverse_mapping.get(s["title"].strip().upper(), s["title"].strip())
                            for s in spans
                            if s.has_attr("title")
                        ]
                    elif key == "ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡":
                        value_container = td.select_one(".main_genre") or td
                        details["å®¹é‡"] = (
                            value_container.get_text(strip=True).replace("æ€»è®¡", "").strip()
                        )

            cover_tag = soup.find("meta", property="og:image")
            if cover_tag:
                details["å°é¢å›¾é“¾æ¥"] = cover_tag["content"]
            if details.get("æ ‡ç­¾"):
                append_new_tags(TAG_JP_PATH, details["æ ‡ç­¾"])

            return {
                "å“ç‰Œ": brand,
                "å‘å”®æ—¥": details.get("å‘å”®æ—¥"),
                "å‰§æœ¬": details.get("å‰§æœ¬", []),
                "åŸç”»": details.get("åŸç”»", []),
                "å£°ä¼˜": details.get("å£°ä¼˜", []),
                "éŸ³ä¹": details.get("éŸ³ä¹", []),
                "æ ‡ç­¾": details.get("æ ‡ç­¾", []),
                "ä½œå“å½¢å¼": details.get("ä½œå“å½¢å¼", []),
                "å°é¢å›¾é“¾æ¥": details.get("å°é¢å›¾é“¾æ¥"),
                "å“ç‰Œé¡µé“¾æ¥": brand_page_url,
                "å®¹é‡": details.get("å®¹é‡"),
            }
        except Exception as e:
            logging.error(f"âŒ [Dlsite] è§£æè¯¦æƒ…é¡µå¤±è´¥: {url} - {e}")
            traceback.print_exc()
            return {}

    async def get_brand_extra_info_with_selenium(self, brand_page_url):
        logging.info("ğŸ” [Dlsite] æ­£åœ¨ç”¨SeleniumæŠ“å–å“ç‰Œé¢å¤–ä¿¡æ¯...")
        if not self.driver:
            raise RuntimeError("DlsiteClientçš„ä¸“å±driveræœªè®¾ç½®ã€‚")
        if not brand_page_url:
            return {}

        def _blocking_task():
            try:
                self.driver.get(brand_page_url)
                try:
                    # å°è¯•è‡ªåŠ¨é€šè¿‡å¹´é¾„éªŒè¯ï¼Œè®¾ç½®è¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                    age_check_wait = WebDriverWait(self.driver, 3)
                    yes_button = age_check_wait.until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, ".btn_yes a"))
                    )
                    yes_button.click()
                    logging.info("ğŸ” [Dlsite] (Selenium) å·²è‡ªåŠ¨é€šè¿‡å¹´é¾„éªŒè¯ã€‚")
                except Exception:
                    pass # å¹´é¾„éªŒè¯ä¸æ˜¯æ¯æ¬¡éƒ½æœ‰ï¼Œå¿½ç•¥å¤±è´¥

                # ä¸»è¦å†…å®¹ç­‰å¾…
                wait = WebDriverWait(self.driver, self.selenium_timeout)
                cien_url = None
                icon_url = None

                try:
                    # ç­–ç•¥1: ç›´æ¥ã€ç‹¬ç«‹åœ°ç­‰å¾…æ¯ä¸ªç›®æ ‡å…ƒç´ åŠ è½½å®Œæˆ
                    cien_link_element = wait.until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='ci-en.dlsite.com']"))
                    )
                    cien_url = cien_link_element.get_attribute("href").strip()
                except TimeoutException:
                    logging.warning("âš ï¸ [Dlsite] (Selenium) åœ¨å“ç‰Œé¡µé¢æœªæ‰¾åˆ° Ci-en é“¾æ¥ã€‚")

                try:
                    icon_img_element = wait.until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".creator_icon img[src]"))
                    )
                    icon_url = icon_img_element.get_attribute("src").strip()
                except TimeoutException:
                    logging.warning("âš ï¸ [Dlsite] (Selenium) åœ¨å“ç‰Œé¡µé¢æœªæ‰¾åˆ°å›¾æ ‡ã€‚")

                if cien_url or icon_url:
                    logging.info(
                        f"âœ… [Dlsite] (Selenium) è·å–æˆåŠŸ: Ci-en={cien_url}, å›¾æ ‡={icon_url}"
                    )
                return {"ci_en_url": cien_url, "icon_url": icon_url}

            except Exception as e:
                logging.error(
                    f"âŒ [Dlsite] (Selenium) æŠ“å–å“ç‰Œä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ {brand_page_url}: {e}"
                )
                return {}

        return await asyncio.to_thread(_blocking_task)
