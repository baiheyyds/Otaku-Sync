# clients/bangumi_client.py
# è¯¥æ¨¡å—ç”¨äºä¸ Bangumi API äº¤äº’ï¼Œè·å–æ¸¸æˆå’Œè§’è‰²ä¿¡æ¯
import difflib
import json
import logging
import os
import re
import time
import unicodedata

import requests

from clients.notion_client import NotionClient
from config.config_fields import FIELDS
from config.config_token import BANGUMI_TOKEN, CHARACTER_DB_ID
from utils.field_helper import extract_aliases, extract_first_valid, extract_link_map

API_TOKEN = BANGUMI_TOKEN
HEADERS_API = {
    "Authorization": f"Bearer {API_TOKEN}",
    "User-Agent": "BangumiSync/1.0",
    "Accept": "application/json",
}

# åŠ è½½å­—æ®µåˆ«åé…ç½®
alias_path = os.path.join(os.path.dirname(__file__), "../config/field_aliases.json")
with open(alias_path, "r", encoding="utf-8") as f:
    FIELD_ALIASES = json.load(f)

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")


def normalize_title(title: str) -> str:
    if not title:
        return ""
    title = unicodedata.normalize("NFKC", title)
    title = title.replace("ï½", "ã€œ").replace("â€™", "'")
    title = title.replace("â€œ", '"').replace("â€", '"')
    title = re.sub(r"[ï¼!]", "!", title)
    title = re.sub(r"[ãƒ¼â”€â”â€•â€â€‘â€’â€“â€”â€•]", "-", title)
    title = re.sub(r"\s+", "", title)
    return title.lower().strip()

def extract_primary_brand_name(name: str) -> str:
    """
    æå–å“ç‰Œåä¸­çš„ä¸»å¹²éƒ¨åˆ†ï¼Œå¿½ç•¥æ‹¬å·ä¸­çš„è¯»éŸ³/æ³¨éŸ³ã€‚
    å¦‚ï¼š'SUKARADOGï¼ˆã‚¹ã‚«ãƒ©ãƒ‰ã‚®ï¼‰' â†’ 'SUKARADOG'
    """
    if not name:
        return name
    # åˆ é™¤ä¸­æ–‡æˆ–è‹±æ–‡æ‹¬å·ä¸­çš„å†…å®¹
    name = re.sub(r"[ï¼ˆ(].*?[ï¼‰)]", "", name)
    return name.strip()


def clean_title(title: str) -> str:
    title = re.sub(r"^ã€.*?ã€‘", "", title)
    title = re.sub(
        r"(é€šå¸¸ç‰ˆ|ä½“é¨“ç‰ˆ|è±ªè¯ç‰ˆ|å®Œå…¨ç‰ˆ|åˆå›é™å®š|é™å®šç‰ˆ|ç‰¹è£…ç‰ˆ|Remake|HD Remaster|æ–°è£…ç‰ˆ|Premium|è±ªè¯çµ¢çˆ›ç‰ˆ|ãƒ‡ãƒ¢)",
        "",
        title,
        flags=re.IGNORECASE,
    )
    return title.strip()


def simplify_title(title: str) -> str:
    return re.split(r"[-â€“~ã€œâ€”â€•]", title)[0].strip()


class BangumiClient:
    def __init__(self, notion: NotionClient):
        self.notion = notion
        self.headers = HEADERS_API
        self.similarity_threshold = 0.85

    def _search(self, keyword: str):
        url = "https://api.bgm.tv/v0/search/subjects"
        payload = {
            "keyword": keyword,
            "sort": "rank",
            "filter": {"type": [4], "nsfw": True},
        }
        resp = requests.post(url, headers=self.headers, json=payload)
        if resp.status_code != 200:
            return []
        return resp.json().get("data", [])

    def search_and_select_bangumi_id(self, keyword: str) -> str | None:
        raw_results = self._search(keyword)
        if not raw_results:
            simplified = simplify_title(keyword)
            if simplified != keyword:
                raw_results = self._search(simplified)
            if not raw_results:
                return None

        # é¢„å¤„ç†å…³é”®è¯
        norm_kw = normalize_title(keyword)
        clean_kw = normalize_title(clean_title(keyword))
        simp_kw = normalize_title(simplify_title(keyword))

        candidates = []
        for item in raw_results:
            name = item.get("name", "")
            name_cn = item.get("name_cn", "")
            norm_name = normalize_title(name)
            norm_cn = normalize_title(name_cn)

            # è®¡ç®—å¤šä¸ªæ–¹å¼çš„æœ€å¤§ç›¸ä¼¼åº¦
            ratios = [
                difflib.SequenceMatcher(None, norm_kw, norm_name).ratio(),
                difflib.SequenceMatcher(None, clean_kw, normalize_title(clean_title(name))).ratio(),
                difflib.SequenceMatcher(None, simp_kw, normalize_title(simplify_title(name))).ratio(),
                difflib.SequenceMatcher(None, norm_kw, norm_cn).ratio(),
            ]
            max_ratio = max(ratios)
            candidates.append((max_ratio, item))

        candidates.sort(key=lambda x: x[0], reverse=True)

        # å­ä¸²åŒ¹é…ï¼ˆæ›´ä¸¥æ ¼ï¼Œåªå…è®¸å€™é€‰æ ‡é¢˜åŒ…å«å…³é”®è¯ï¼‰
        for _, item in candidates:
            item_clean = clean_title(item.get("name", ""))
            keyword_clean = clean_title(keyword)
            # åªå…è®¸å…³é”®è¯æ˜¯å­ä¸²ï¼Œå€™é€‰æ ‡é¢˜åŒ…å«å…³é”®è¯æ‰åŒ¹é…
            if item_clean and (keyword_clean in item_clean):
                logging.info(f"å­ä¸²åŒ¹é…æˆåŠŸï¼š{item['name']}ï¼Œè§†ä¸ºåŒä¸€ä½œå“")
                return str(item["id"])


        # âœ… ç›¸ä¼¼åº¦ â‰¥ é˜ˆå€¼
        if candidates and candidates[0][0] >= self.similarity_threshold:
            best = candidates[0][1]
            logging.info(f"è‡ªåŠ¨åŒ¹é… Bangumi: {best['name']}ï¼ˆç›¸ä¼¼åº¦ {candidates[0][0]:.2f}ï¼‰")
            return str(best["id"])

        # âœ… å®½æ¾åŒ¹é…ï¼šæ ‡é¢˜åŒ…å«æˆ–ç›¸ä¼¼åº¦ç¨ä½
        if candidates and candidates[0][0] >= 0.7:
            best = candidates[0][1]
            if clean_title(best["name"]) in clean_title(keyword) or clean_title(keyword) in clean_title(best["name"]):
                logging.info(f"æ¨¡ç³ŠåŒ¹é… Bangumiï¼ˆæ”¾å®½åˆ¤å®šï¼‰: {best['name']}ï¼ˆç›¸ä¼¼åº¦ {candidates[0][0]:.2f}ï¼‰")
                return str(best["id"])

        # âŒ æ— æ³•è‡ªåŠ¨åŒ¹é…ï¼Œè½¬æ‰‹åŠ¨é€‰æ‹©
        print("âš ï¸ Bangumiè‡ªåŠ¨åŒ¹é…ç›¸ä¼¼åº¦ä¸è¶³ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹©:")
        for idx, (ratio, item) in enumerate(candidates[:10]):
            print(f"{idx + 1}. {item['name']} / {item.get('name_cn','')} (ç›¸ä¼¼åº¦: {ratio:.2f})")
        print("0. æ”¾å¼ƒåŒ¹é…")

        while True:
            sel = input("è¯·è¾“å…¥åºå·é€‰æ‹© Bangumi æ¡ç›®ï¼ˆ0æ”¾å¼ƒï¼‰ï¼š").strip()
            if sel.isdigit():
                sel_int = int(sel)
                if sel_int == 0:
                    return None
                if 1 <= sel_int <= len(candidates):
                    return str(candidates[sel_int - 1][1]["id"])
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

    def fetch_game(self, subject_id: str) -> dict:
        url = f"https://api.bgm.tv/v0/subjects/{subject_id}"
        r = requests.get(url, headers=self.headers)
        if r.status_code != 200:
            return {}
        d = r.json()
        # ä» images å­—æ®µä¼˜å…ˆå– largeï¼Œfallbackç”¨ image
        cover_url = d.get("images", {}).get("large") or d.get("image") or ""
        return {
            "title": d.get("name"),
            "title_cn": d.get("name_cn"),
            "release_date": d.get("date"),
            "summary": d.get("summary", ""),
            "url": f"https://bangumi.tv/subject/{subject_id}",
            "å°é¢å›¾é“¾æ¥": cover_url,
        }


    def fetch_characters(self, subject_id: str) -> list:
        url = f"https://api.bgm.tv/v0/subjects/{subject_id}/characters"
        r = requests.get(url, headers=self.headers)
        if r.status_code != 200:
            return []

        characters = []
        for ch in r.json():
            char_id = ch["id"]
            detail_resp = requests.get(f"https://api.bgm.tv/v0/characters/{char_id}", headers=self.headers)
            if detail_resp.status_code != 200:
                continue
            detail = detail_resp.json()
            raw_stats = detail.get("infobox", [])
            stats = {}
            for item in raw_stats:
                key = item.get("key")
                val = item.get("value")
                if not key or val is None:
                    continue
                for canonical, aliases in FIELD_ALIASES.items():
                    if key in aliases:
                        if isinstance(val, list):
                            stats[canonical] = ", ".join([f"{i['k']}: {i['v']}" for i in val if isinstance(i, dict)])
                        else:
                            stats[canonical] = val
                        break

            aliases = set()
            if detail.get("name_cn"):
                aliases.add(detail["name_cn"])
            alias_raw = stats.get("character_alias")
            if isinstance(alias_raw, str):
                aliases.update([a.strip() for a in re.split(r"[ã€,ï¼Œ/ï¼ï¼›;]", alias_raw)])
            elif isinstance(alias_raw, list):
                for item in alias_raw:
                    val = item.strip() if isinstance(item, str) else item.get("v", "").strip()
                    aliases.add(val)

            characters.append(
                {
                    "name": detail["name"],
                    "cv": ch["actors"][0]["name"] if ch.get("actors") else "",
                    "avatar": detail.get("images", {}).get("large", ""),
                    "summary": detail.get("summary", "").strip(),
                    "bwh": stats.get("character_bwh", ""),
                    "height": stats.get("character_height", ""),
                    "gender": stats.get("character_gender", ""),
                    "birthday": stats.get("character_birthday", ""),
                    "blood_type": stats.get("character_blood_type", ""),
                    "url": f"https://bangumi.tv/character/{char_id}",
                    "aliases": list(aliases),
                }
            )
        return characters

    def _character_exists(self, url: str) -> str | None:
        query_url = f"https://api.notion.com/v1/databases/{CHARACTER_DB_ID}/query"
        payload = {"filter": {"property": "è¯¦æƒ…é¡µé¢", "url": {"equals": url}}}
        resp = self.notion._request("POST", query_url, payload)
        results = resp.get("results", []) if resp else []
        return results[0]["id"] if results else None

    def create_or_update_character(self, char: dict) -> str | None:
        existing_id = self._character_exists(char["url"])
        if existing_id:
            logging.info(f"è§’è‰²å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºï¼š{char['name']}")
            return existing_id

        props = {
            "è§’è‰²åç§°": {"title": [{"text": {"content": char["name"]}}]},
            "è¯¦æƒ…é¡µé¢": {"url": char["url"]},
        }
        if char.get("cv"):
            props["å£°ä¼˜"] = {"rich_text": [{"text": {"content": char["cv"]}}]}
        if char.get("gender"):
            props["æ€§åˆ«"] = {"select": {"name": char["gender"]}}
        if char.get("bwh"):
            props["BWH"] = {"rich_text": [{"text": {"content": char["bwh"]}}]}
        if char.get("height"):
            props[FIELDS["character_height"]] = {"rich_text": [{"text": {"content": char["height"]}}]}
        if char.get("birthday"):
            props[FIELDS["character_birthday"]] = {"rich_text": [{"text": {"content": char["birthday"]}}]}
        if char.get("blood_type"):
            props[FIELDS["character_blood_type"]] = {"select": {"name": char["blood_type"]}}
        if char.get("summary"):
            props["ç®€ä»‹"] = {"rich_text": [{"text": {"content": char["summary"]}}]}
        if char.get("avatar"):
            props["å¤´åƒ"] = {
                "files": [
                    {
                        "type": "external",
                        "name": "avatar",
                        "external": {"url": char["avatar"]},
                    }
                ]
            }
        if char.get("aliases"):
            alias_text = "ã€".join(char["aliases"][:20])
            props["åˆ«å"] = {"rich_text": [{"text": {"content": alias_text}}]}

        payload = {"parent": {"database_id": CHARACTER_DB_ID}, "properties": props}
        r = requests.post("https://api.notion.com/v1/pages", headers=self.notion.headers, json=payload)
        return r.json().get("id") if r.status_code == 200 else None

    def create_or_link_characters(self, game_page_id: str, subject_id: str):
        characters = self.fetch_characters(subject_id)
        character_relations = []
        all_cvs = set()
        for ch in characters:
            char_id = self.create_or_update_character(ch)
            if char_id:
                character_relations.append({"id": char_id})
            if ch.get("cv"):
                all_cvs.add(ch["cv"].strip())
            time.sleep(0.3)

        patch = {
            FIELDS["bangumi_url"]: {"url": f"https://bangumi.tv/subject/{subject_id}"},
            FIELDS["game_characters"]: {"relation": character_relations},
        }
        if all_cvs:
            patch[FIELDS["voice_actor"]] = {"multi_select": [{"name": name} for name in sorted(all_cvs)]}
        self.notion._request(
            "PATCH",
            f"https://api.notion.com/v1/pages/{game_page_id}",
            {"properties": patch},
        )
        logging.info("Bangumiè§’è‰²ä¿¡æ¯åŒæ­¥å®Œæˆ")

    def fetch_brand_info_from_bangumi(self, brand_name: str) -> dict | None:
        def search_brand(keyword: str):
            print(f"ğŸ” æ­£åœ¨æœç´¢ Bangumi å“ç‰Œå…³é”®è¯: {keyword}")
            url = "https://api.bgm.tv/v0/search/persons"
            headers = {
                "Authorization": f"Bearer {BANGUMI_TOKEN}",
                "Content-Type": "application/json",
                "User-Agent": "OtakuNotionSync/1.0",
            }
            data = {
                "keyword": keyword,
                "filter": {"career": ["artist", "director", "producer"]},
            }
            resp = requests.post(url, headers=headers, json=data)
            if resp.status_code != 200:
                print(f"âŒ Bangumi æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}ï¼Œå“åº”: {resp.text}")
                return []
            results = resp.json().get("data", [])
            print(f"âœ… æœç´¢ç»“æœæ•°: {len(results)}")
            return results

        primary_name = extract_primary_brand_name(brand_name)
        print(f"ğŸ¯ æå–ä¸»å“ç‰Œå: {primary_name}")

        # åªç”¨ä¸»å“ç‰Œåä½œä¸ºæœç´¢å…³é”®è¯ï¼Œé¿å…é‡å¤æœç´¢å¸¦æ‹¬å·çš„åå­—
        search_keywords = [primary_name] if primary_name else [brand_name]

        best_match, best_score = None, 0
        for keyword in search_keywords:
            results = search_brand(keyword)
            for r in results:
                candidate_name = r.get("name", "")
                infobox = r.get("infobox", [])
                aliases = extract_aliases(infobox)
                names = [candidate_name] + aliases
                score = max(difflib.SequenceMatcher(None, brand_name.lower(), n.lower()).ratio() for n in names)
                print(f"ğŸ§ª å€™é€‰: {candidate_name} | ç›¸ä¼¼åº¦: {score:.2f} | åˆ«å: {aliases}")
                if score > best_score:
                    best_score = score
                    best_match = r
            if best_score >= 0.85:
                print(f"âœ… æå‰åŒ¹é…æˆåŠŸ: {best_match.get('name')} (å¾—åˆ†: {best_score:.2f})")
                break

        if not best_match or best_score < 0.7:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„å“ç‰Œï¼ˆæœ€é«˜: {best_score:.2f}ï¼‰")
            return None

        infobox = best_match.get("infobox", [])
        links = extract_link_map(infobox)
        summary = best_match.get("summary", "")
        icon_url = best_match.get("img")
        birthday = extract_first_valid(infobox, FIELD_ALIASES.get("brand_birthday", []))
        company_address = extract_first_valid(infobox, ["å…¬å¸åœ°å€", "åœ°å€", "æ‰€åœ¨åœ°", "æ‰€åœ¨åœ°åœ°å€"])
        bangumi_url = f"https://bgm.tv/person/{best_match['id']}" if best_match.get("id") else None
        aliases = extract_aliases(infobox)

        twitter = links.get("brand_twitter") or links.get("Twitter") or ""
        if twitter.startswith("@"):
            twitter = f"https://twitter.com/{twitter[1:]}"

        print(f"âœ… æœ€ç»ˆåŒ¹é…å“ç‰Œ: {best_match.get('name')} (ID: {best_match.get('id')})")

        return {
            "summary": summary,
            "icon": icon_url,
            "birthday": birthday,
            "company_address": company_address,
            "homepage": links.get("brand_official_url") or links.get("å®˜ç½‘"),
            "twitter": twitter,
            "bangumi_url": bangumi_url,
            "alias": aliases,
        }
