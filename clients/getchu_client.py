# clients/getchu_client.py
# GetchuClient ç±»ç”¨äºæŠ“å– Getchu ç½‘ç«™çš„æ¸¸æˆä¿¡æ¯
import random
import re
import time
from urllib.parse import quote, urljoin

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait


class GetchuClient:
    BASE_URL = "https://www.getchu.com"
    SEARCH_URL = "https://www.getchu.com/php/nsearch.phtml"

    def __init__(self, driver=None, driver_path=None, headless=True):
        self.driver = driver
        self.headless = headless
        self.driver_path = driver_path
        self.session = requests.Session()

        if not self.driver:
            self._init_driver()

    def _init_driver(self):
        if self.driver is not None:
            return
        options = Options()
        if self.headless:
            options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--log-level=3")
        service = ChromeService(executable_path=self.driver_path) if self.driver_path else ChromeService()
        self.driver = webdriver.Chrome(service=service, options=options)

    def _get_headers(self):
        headers = {
            "User-Agent": random.choice(
                [
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.67 Safari/537.36",
                ]
            ),
            "Referer": "https://www.getchu.com/",
        }
        return headers

    def search(self, keyword):
        print("ğŸ” [Getchu] å¼€å§‹æœç´¢...")
        try:
            safe_keyword = keyword.replace("ï½", "ã€œ")
            encoded_keyword = quote(safe_keyword.encode("shift_jis"))
            url = f"{self.SEARCH_URL}?genre=all&search_keyword={encoded_keyword}&check_key_dtl=1&submit="
            retries = 5
            while retries > 0:
                resp = self.session.get(url, timeout=10, headers=self._get_headers())
                if resp.status_code == 403:
                    retries -= 1
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {retries}")
                    time.sleep(2)
                else:
                    break

            if resp.status_code != 200:
                print(f"âŒ æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
                return []

            resp.encoding = "euc_jp"
            soup = BeautifulSoup(resp.text, "html.parser")
            result_ul = soup.find("ul", class_="display")
            if not result_ul:
                print("âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœåŒºåŸŸã€‚")
                return []

            items = []
            for li in result_ul.find_all("li"):
                block = li.select_one("#detail_block")
                if not block:
                    continue

                title_tag = block.select_one("a.blueb[href*='soft.phtml?id=']")

                # æå–ç±»å‹
                type_tag = block.select_one("span.orangeb")
                item_type = type_tag.get_text(strip=True) if type_tag else "æœªçŸ¥"

                # ğŸ› ï¸ å¢å¼ºç‰ˆä»·æ ¼æå–é€»è¾‘
                price_tag = block.select_one(".redb")
                if price_tag:
                    price = price_tag.get_text(strip=True)
                else:
                    price = "æœªçŸ¥"
                    for p in block.find_all("p"):
                        text = p.get_text()
                        if "å®šä¾¡" in text:
                            match = re.search(r"å®šä¾¡[:ï¼š]?\s*([^\s<ï¼ˆ]+)", text)
                            if match:
                                price = match.group(1)
                                break

                if title_tag:
                    game_id = re.search(r"id=(\d+)", title_tag["href"])
                    if not game_id:
                        continue
                    items.append(
                        {
                            "title": title_tag.get_text(strip=True),
                            "url": f"{self.BASE_URL}/soft.phtml?id={game_id.group(1)}",
                            "ä»·æ ¼": price,
                            "ç±»å‹": item_type,  # âœ… æ–°å¢å­—æ®µ
                        }
                    )

            # æ–°å¢ï¼šè¿‡æ»¤åªä¿ç•™æ¸¸æˆç±»å‹
            items = [item for item in items if "ã‚²ãƒ¼ãƒ " in item.get("ç±»å‹", "")]

            exclude_keywords = ["ã‚°ãƒƒã‚º", "BOOKS", "CD", "éŸ³æ¥½"]
            items = [item for item in items if not any(ex_kw in item.get("ç±»å‹", "") for ex_kw in exclude_keywords)]

            print(f"âœ… æ‰¾åˆ° {len(items)} ä¸ªæœç´¢ç»“æœã€‚")
            return items

        except UnicodeEncodeError as ue:
            print(f"âŒ ç¼–ç å¤±è´¥ï¼š{ue}")
            return []
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥ï¼š{e}")
            return []

    def get_game_detail(self, url):
        try:
            print(f"\nğŸš€ æ­£åœ¨åŠ è½½æ¸¸æˆè¯¦æƒ…é¡µ: {url}")
            self.driver.get(url)
            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

            if "å¹´é½¢èªè¨¼" in self.driver.title:
                print("âš ï¸ é‡åˆ°å¹´é¾„éªŒè¯ï¼Œå°è¯•é€šè¿‡...")
                links = self.driver.find_elements(By.TAG_NAME, "a")
                for link in links:
                    if "ã™ã™ã‚€" in link.text:
                        link.click()
                        WebDriverWait(self.driver, 10).until(EC.title_contains("Getchu"))
                        break

            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            info_table = soup.find("table", {"width": "100%", "style": "padding:1px;"})

            def extract_info(keyword):
                if not info_table:
                    return None
                for tr in info_table.find_all("tr"):
                    tds = tr.find_all("td")
                    if len(tds) >= 2 and keyword in tds[0].get_text(strip=True):
                        return tds[1].get_text("ã€", strip=True)
                return None

            img_tag = soup.select_one("a.highslide > img")
            raw_image_url = urljoin(self.BASE_URL, img_tag["src"]) if img_tag and img_tag.get("src") else None
            image_url = raw_image_url.replace(self.BASE_URL, "https://cover.ydgal.com") if raw_image_url else None

            # --- æ–°ç»“æ„çš„å“ç‰Œä¿¡æ¯æå– ---
            brand, brand_site = None, None
            trs = soup.find_all("tr")
            for tr in trs:
                tds = tr.find_all("td")
                if len(tds) >= 2 and "ãƒ–ãƒ©ãƒ³ãƒ‰" in tds[0].get_text(strip=True):
                    a_tags = tds[1].find_all("a")
                    if a_tags:
                        brand = a_tags[0].get_text(strip=True)
                        brand_site = a_tags[0]["href"]
                    break

            title_tag = soup.select_one("title")
            title = title_tag.text.strip().split(" (")[0] if title_tag else None

            print("ğŸ“¦ æ­£åœ¨æå–å­—æ®µ...")

            result = {
                "å°é¢å›¾é“¾æ¥": image_url,
                "æ ‡é¢˜": title,
                "å“ç‰Œ": brand,
                "å“ç‰Œå®˜ç½‘": brand_site,
                "å‘å”®æ—¥": extract_info("ç™ºå£²æ—¥"),
                "ä»·æ ¼": extract_info("å®šä¾¡"),
                "åŸç”»": extract_info("åŸç”»"),
                "å‰§æœ¬": extract_info("ã‚·ãƒŠãƒªã‚ª"),
            }

            print("\nğŸ¯ æŠ“å–ç»“æœ:")
            for k, v in result.items():
                print(f"{k}: {v}")

            return result

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return {}

    def close(self):
        if self.driver:
            print("ğŸ§¹ æ­£åœ¨å…³é—­æµè§ˆå™¨é©±åŠ¨...")
            self.driver.quit()
            self.driver = None


if __name__ == "__main__":
    client = GetchuClient(headless=True)
    try:
        while True:
            keyword = input("\nè¯·è¾“å…¥æ¸¸æˆå…³é”®è¯ï¼ˆæ—¥æ–‡ï¼Œå›è½¦é€€å‡ºï¼‰ï¼š").strip()
            if not keyword:
                break
            results = client.search(keyword)
            if not results:
                print("æ²¡æœ‰æœç´¢ç»“æœã€‚")
                continue
            for i, item in enumerate(results):
                print(f"[{i}] ğŸ® {item['title']} | ğŸ’´ {item['ä»·æ ¼']}")
            idx = input("è¯·é€‰æ‹©æ¸¸æˆç¼–å·æŸ¥çœ‹è¯¦æƒ…ï¼ˆå›è½¦è·³è¿‡ï¼‰ï¼š").strip()
            if idx.isdigit() and 0 <= int(idx) < len(results):
                client.get_game_detail(results[int(idx)]["url"])
    finally:
        client.close()
        client.close()
