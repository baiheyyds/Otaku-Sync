# clients/getchu_client.py
# GetchuClient ç±»ç”¨äºæŠ“å– Getchu ç½‘ç«™çš„æ¸¸æˆä¿¡æ¯
import random
import re
from urllib.parse import quote, urljoin

import requests
from bs4 import BeautifulSoup


class GetchuClient:
    BASE_URL = "https://www.getchu.com"
    SEARCH_URL = "https://www.getchu.com/php/nsearch.phtml"
    # AGE_CHECK_URL ä¸å†éœ€è¦ï¼Œå› ä¸ºæˆ‘ä»¬é‡‡ç”¨æ›´ç›´æ¥çš„æ–¹å¼

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(self._get_headers())
        # åœ¨åˆå§‹åŒ–æ—¶å°±å®Œæˆå¹´é¾„è®¤è¯
        self._perform_age_check()

    def _perform_age_check(self):
        """
        ä¿®æ”¹ï¼šç›´æ¥åœ¨sessionä¸­æ‰‹åŠ¨è®¾ç½®å¹´é¾„éªŒè¯Cookieï¼Œè¿™æ˜¯æœ€å¯é çš„æ–¹å¼ã€‚
        """
        try:
            print("â³ [Getchu] æ­£åœ¨æ‰‹åŠ¨è®¾ç½®å¹´é¾„è®¤è¯Cookie...")
            self.session.cookies.set(
                name="getchu_adalt_flag",
                value="getchu.com",
                domain=".getchu.com",  # ä½¿ç”¨ .getchu.com ä¿è¯å¯¹æ‰€æœ‰å­åŸŸåæœ‰æ•ˆ
            )
            print("âœ… [Getchu] 'getchu_adalt_flag' Cookieå·²æ‰‹åŠ¨è®¾ç½®ã€‚")
        except Exception as e:
            print(f"âŒ [Getchu] æ‰‹åŠ¨è®¾ç½®Cookieå¤±è´¥: {e}")

    def _get_headers(self):
        headers = {
            "User-Agent": random.choice(
                [
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36",
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.67 Safari/537.36",
                ]
            ),
            "Referer": "https://www.getchu.com/",
        }
        return headers

    def search(self, keyword):
        print("ğŸ” [Getchu] å¼€å§‹æœç´¢...")
        try:
            safe_keyword = keyword.replace("ï½", "ã€œ")
            encoded_keyword = quote(safe_keyword.encode("shift_jis"))
            url = f"{self.SEARCH_URL}?genre=all&search_keyword={encoded_keyword}&check_key_dtl=1&submit="

            resp = self.session.get(url, timeout=10)
            resp.raise_for_status()

            resp.encoding = "euc_jp"
            soup = BeautifulSoup(resp.text, "html.parser")
            result_ul = soup.find("ul", class_="display")
            if not result_ul:
                print("âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœåŒºåŸŸã€‚")
                return []

            items = []
            for li in result_ul.find_all("li"):
                block = li.select_one("#detail_block")
                if not block:
                    continue

                title_tag = block.select_one("a.blueb[href*='soft.phtml?id=']")
                type_tag = block.select_one("span.orangeb")
                item_type = type_tag.get_text(strip=True) if type_tag else "æœªçŸ¥"
                price_tag = block.select_one(".redb")

                if price_tag:
                    price = price_tag.get_text(strip=True)
                else:
                    price = "æœªçŸ¥"
                    for p in block.find_all("p"):
                        text = p.get_text()
                        if "å®šä¾¡" in text:
                            match = re.search(r"å®šä¾¡[:ï¼š]?\s*([^\s<ï¼ˆ]+)", text)
                            if match:
                                price = match.group(1)
                                break

                if title_tag:
                    game_id = re.search(r"id=(\d+)", title_tag["href"])
                    if not game_id:
                        continue
                    items.append(
                        {
                            "title": title_tag.get_text(strip=True),
                            "url": f"{self.BASE_URL}/soft.phtml?id={game_id.group(1)}",
                            "ä»·æ ¼": price,
                            "ç±»å‹": item_type,
                        }
                    )

            items = [item for item in items if "ã‚²ãƒ¼ãƒ " in item.get("ç±»å‹", "")]
            exclude_keywords = ["ã‚°ãƒƒã‚º", "BOOKS", "CD", "éŸ³æ¥½"]
            items = [item for item in items if not any(ex_kw in item.get("ç±»å‹", "") for ex_kw in exclude_keywords)]

            print(f"âœ… æ‰¾åˆ° {len(items)} ä¸ªæœç´¢ç»“æœã€‚")
            return items

        except UnicodeEncodeError as ue:
            print(f"âŒ ç¼–ç å¤±è´¥ï¼š{ue}")
            return []
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥ï¼š{e}")
            return []

    def get_game_detail(self, url):
        try:
            print(f"\nğŸš€ [Getchu] æ­£åœ¨é€šè¿‡requestsåŠ è½½æ¸¸æˆè¯¦æƒ…é¡µ: {url}")
            # å› ä¸ºcookieå·²ç»è®¾ç½®å¥½ï¼Œè¿™ä¸ªè¯·æ±‚ç°åœ¨ä¼šç›´æ¥å‘½ä¸­è¯¦æƒ…é¡µ
            resp = self.session.get(url, timeout=15)
            resp.encoding = "euc_jp"
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "html.parser")

            # å¦‚æœæˆåŠŸï¼Œæ ‡é¢˜ä¸ä¼šæ˜¯'Getchu.comï¼šR18 å¹´é½¢èªè¨¼'
            if "å¹´é½¢èªè¨¼" in soup.title.text:
                print("âŒ [Getchu] ç»•è¿‡å¹´é¾„éªŒè¯å¤±è´¥ï¼Œé¡µé¢å†…å®¹ä¸æ­£ç¡®ã€‚")
                return {}

            info_table = soup.find("table", {"width": "100%", "style": "padding:1px;"})

            def extract_info(keyword):
                if not info_table:
                    return None
                for tr in info_table.find_all("tr"):
                    tds = tr.find_all("td")
                    if len(tds) >= 2 and keyword in tds[0].get_text(strip=True):
                        return tds[1].get_text("ã€", strip=True)
                return None

            img_tag = soup.select_one("a.highslide > img")
            raw_image_url = urljoin(self.BASE_URL, img_tag["src"]) if img_tag and img_tag.get("src") else None
            image_url = raw_image_url.replace(self.BASE_URL, "https://cover.ydgal.com") if raw_image_url else None

            brand, brand_site = None, None
            trs = soup.find_all("tr")
            for tr in trs:
                tds = tr.find_all("td")
                if len(tds) >= 2 and "ãƒ–ãƒ©ãƒ³ãƒ‰" in tds[0].get_text(strip=True):
                    a_tags = tds[1].find_all("a")
                    if a_tags:
                        brand = a_tags[0].get_text(strip=True)
                        brand_site = a_tags[0]["href"]
                    break

            title_tag = soup.select_one("title")
            title = title_tag.text.strip().split(" (")[0] if title_tag else None

            print("ğŸ“¦ æ­£åœ¨æå–å­—æ®µ...")
            result = {
                "å°é¢å›¾é“¾æ¥": image_url,
                "æ ‡é¢˜": title,
                "å“ç‰Œ": brand,
                "å“ç‰Œå®˜ç½‘": brand_site,
                "å‘å”®æ—¥": extract_info("ç™ºå£²æ—¥"),
                "ä»·æ ¼": extract_info("å®šä¾¡"),
                "åŸç”»": extract_info("åŸç”»"),
                "å‰§æœ¬": extract_info("ã‚·ãƒŠãƒªã‚ª"),
            }

            print("\nğŸ¯ æŠ“å–ç»“æœ:")
            for k, v in result.items():
                print(f"{k}: {v}")

            return result

        except Exception as e:
            print(f"âŒ (requests)æŠ“å–å¤±è´¥: {e}")
            return {}
