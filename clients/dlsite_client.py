# clients/dlsite_client.py
# è¯¥æ¨¡å—ç”¨äºä¸ Dlsite ç½‘ç«™äº¤äº’ï¼Œè·å–æ¸¸æˆä¿¡æ¯å’Œå“ç‰Œæ•°æ®
import os
import urllib.parse

import requests
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from utils.tag_logger import append_new_tags

TAG_JP_PATH = os.path.join(os.path.dirname(__file__), "..", "mapping", "tag_jp_to_cn.json")


class DlsiteClient:
    BASE_URL = "https://www.dlsite.com"

    def __init__(self, headers=None):
        self.headers = headers or {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.dlsite.com/maniax/",
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.driver = None

    def set_driver(self, driver):
        """å¤–éƒ¨æ³¨å…¥driverçš„æ–¹æ³•"""
        self.driver = driver

    def search(self, keyword, limit=30):
        print(f"ğŸ” [Dlsite] æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
        query = urllib.parse.quote_plus(keyword)
        url = (
            f"{self.BASE_URL}/maniax/fsr/=/language/jp/"
            f"sex_category%5B0%5D/male/"
            f"keyword/{query}/"
            f"work_category%5B0%5D/doujin/"
            f"work_category%5B1%5D/books/"
            f"work_category%5B2%5D/pc/"
            f"work_category%5B3%5D/app/"
            f"order%5B0%5D/trend/"
            f"options_and_or/and/"
            f"per_page/30/"
            f"page/1/"
            f"from/fs.header"
        )
        r = self.session.get(url, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        results = []
        seen = set()

        for a in soup.select("a[href*='/work/=/product_id/']"):
            container = a.find_parent()
            for _ in range(3):
                if container is None:
                    break
                price_tag = container.select_one(".price, .work_price, .price_display")
                if price_tag:
                    break
                container = container.parent
            price = price_tag.get_text(strip=True) if price_tag else "æ— "
            title = a.get("title", "").strip()
            href = a["href"]
            full_url = href if href.startswith("http") else self.BASE_URL + href
            li_tag = a.find_parent("li", class_="search_result_img_box_inner")
            work_type_tag = li_tag.select_one(".work_category a") if li_tag else None
            work_type = work_type_tag.get_text(strip=True) if work_type_tag else None

            if title and full_url and full_url not in seen:
                results.append({"title": title, "url": full_url, "price": price, "ç±»å‹": work_type})
                seen.add(full_url)
            if len(results) >= limit:
                break

        exclude_keywords = ["å˜è¡Œæœ¬", "ãƒãƒ³ã‚¬", "å°èª¬", "æ›¸ç±", "é›‘èªŒ/ã‚¢ãƒ³ã‚½ãƒ­", "ãƒœã‚¤ã‚¹ãƒ»ASMR", "éŸ³æ¥½", "å‹•ç”»"]
        filtered_results = [
            item for item in results if not any(ex_kw in item.get("ç±»å‹", "") for ex_kw in exclude_keywords)
        ]

        print(f"âœ… [Dlsite] ç­›é€‰åæ‰¾åˆ° {len(filtered_results)} æ¡æ¸¸æˆç›¸å…³ç»“æœ")
        return filtered_results

    def get_game_detail(self, url):
        r = self.session.get(url, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")

        brand_tag = soup.select_one("#work_maker .maker_name a")
        brand = brand_tag.get_text(strip=True) if brand_tag else None
        brand_page_url = brand_tag["href"] if brand_tag and brand_tag.has_attr("href") else None
        if brand_page_url and not brand_page_url.startswith("http"):
            brand_page_url = self.BASE_URL + brand_page_url

        sale_date, scenario, illustrator, voice_actor, music, genres, work_type, capacity = (
            None,
            [],
            [],
            [],
            [],
            [],
            [],
            None,
        )
        table = soup.find("table", id="work_outline")
        if table:
            for tr in table.find_all("tr"):
                th, td = tr.find("th"), tr.find("td")
                if not th or not td:
                    continue
                key = th.get_text(strip=True)
                if key == "è²©å£²æ—¥":
                    sale_date = td.get_text(strip=True)
                elif key == "ã‚·ãƒŠãƒªã‚ª":
                    scenario = [a.get_text(strip=True) for a in td.find_all("a")]
                elif key == "ã‚¤ãƒ©ã‚¹ãƒˆ":
                    illustrator = [a.get_text(strip=True) for a in td.find_all("a")]
                elif key == "å£°å„ª":
                    voice_actor = [a.get_text(strip=True) for a in td.find_all("a")]
                elif key == "éŸ³æ¥½":
                    music = [a.get_text(strip=True) for a in td.find_all("a")]
                elif key == "ã‚¸ãƒ£ãƒ³ãƒ«":
                    genres = [a.get_text(strip=True) for a in td.find_all("a")]
                elif key == "ä½œå“å½¢å¼":
                    spans = td.find_all("span", title=True)
                    mapping = {
                        "ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ³ã‚°": "RPG",
                        "ã‚¢ãƒ‰ãƒ™ãƒ³ãƒãƒ£ãƒ¼": "ADV",
                        "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "æ¨¡æ‹Ÿ",
                        "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³": "ACT",
                        "éŸ³å£°ã‚ã‚Š": "æœ‰å£°éŸ³",
                        "éŸ³æ¥½ã‚ã‚Š": "æœ‰éŸ³ä¹",
                        "å‹•ç”»ã‚ã‚Š": "æœ‰åŠ¨ç”»",
                    }
                    work_type = [mapping.get(s["title"].strip(), s["title"].strip()) for s in spans]
                elif key == "ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡":
                    raw_text = td.get_text(strip=True)
                    capacity = raw_text.replace("ç·è¨ˆ", "").strip()

        cover = soup.find("meta", property="og:image")["content"] if soup.find("meta", property="og:image") else None
        if genres:
            append_new_tags(TAG_JP_PATH, genres)

        return {
            "å“ç‰Œ": brand,
            "å‘å”®æ—¥": sale_date,
            "å‰§æœ¬": scenario,
            "åŸç”»": illustrator,
            "å£°ä¼˜": voice_actor,
            "éŸ³ä¹": music,
            "æ ‡ç­¾": genres,
            "ä½œå“å½¢å¼": work_type,
            "å°é¢å›¾é“¾æ¥": cover,
            "å“ç‰Œé¡µé“¾æ¥": brand_page_url,
            "å®¹é‡": capacity,
        }

    def get_brand_extra_info_with_selenium(self, brand_page_url):
        print(f"ğŸ”© [Dlsite] æ­£åœ¨å¯åŠ¨SeleniumæŠ“å–å“ç‰Œé¢å¤–ä¿¡æ¯...")
        if not self.driver:
            raise RuntimeError("DlsiteClientçš„Selenium driveræœªè®¾ç½®ï¼Œæ— æ³•æ‰§è¡ŒJSæ¸²æŸ“æŠ“å–ã€‚")
        if not brand_page_url:
            return {"å®˜ç½‘": None, "å›¾æ ‡": None}
        try:
            self.driver.get(brand_page_url)
            wait = WebDriverWait(self.driver, 10)
            link_block_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.link_cien")))
            soup = BeautifulSoup(link_block_element.get_attribute("outerHTML"), "html.parser")
            cien_link = soup.select_one("a[href]")
            icon_img = soup.select_one("img[src]")
            official_url = cien_link["href"].strip() if cien_link else None
            icon_url = icon_img["src"].strip() if icon_img else None
            print(f"âœ… [Dlsite] (Selenium)è·å–æˆåŠŸ: å®˜ç½‘={official_url}, å›¾æ ‡={icon_url}")
            return {"å®˜ç½‘": official_url, "å›¾æ ‡": icon_url}
        except Exception as e:
            print(f"âŒ [Dlsite] (Selenium)æŠ“å–å“ç‰Œä¿¡æ¯å¤±è´¥ {brand_page_url}: {e}")
            return {"å®˜ç½‘": None, "å›¾æ ‡": None}
